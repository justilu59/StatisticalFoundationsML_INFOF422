{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOP7yVJ8Q7VA"
      },
      "source": [
        "# Introduction to Machine-Learning Practical\n",
        "In this practical I will try to show you how to create a machine-learning model to predict the diabetes risk of a person.  \n",
        "We will perform data exploration, data processing, model training and evaluation and finally, fine-tuning.  \n",
        "At different parts of this practical I will add screenshots of the lecture that I did at the beginning of the year to remind you of the concepts.  \n",
        "\n",
        "### Context\n",
        "We will use the Diabetes dataset, a survey of more than 70,000 american people with 17 measurements per person and if they have diabetes or not. For the sake of this practical I reduced the dataset to ~5,500 people.  \n",
        "**Today we will try to build a machine-learning model that can predict if a person will likely develop diabetes or not.**  \n",
        "If you want more details on the data you can find  them [here](https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset)  \n",
        "\n",
        "<details><summary>We will use a total of 17 measurements to predict the diabete risk. Click here to open the meaning of each column of the file</summary>\n",
        "<p>\n",
        " <ul>\n",
        "  <li><strong>Age:</strong> age of the person [18-95]</li>\n",
        "  <li><strong>Sex:</strong> sex of the person [male, female]</li>\n",
        "  <li><strong>High Chol:</strong> high level of cholesterol [yes, no]</li>\n",
        "  <li><strong>CholCheck:</strong> cholesterol check in last 5 years [yes, no]</li>\n",
        "  <li><strong>BMI:</strong> body mass index [15-100]</li>\n",
        "  <li><strong>Smoker:</strong> does the person smoke ? [0-1]</li>\n",
        "  <li><strong>HeartDiseaseorAttack:</strong> had heart issues [0-1]</li>\n",
        "  <li><strong>PhysActivity:</strong> physical activity past 30 days [0-1]</li>\n",
        "  <li><strong>Fruits:</strong> eat fruit every days [0-1]</li>\n",
        "  <li><strong>Veggies:</strong> eat veggies every days [0-1]</li>\n",
        "  <li><strong>HvyAlcoholConsump:</strong> heavy alcohol drinker [0-1]</li>\n",
        "  <li><strong>GenHlth:</strong> self-assessed general health level [excellent, very good, good, fair, poor]</li>\n",
        "  <li><strong>MentHlth:</strong> days of poor mental health during last month [0-30]</li>\n",
        "  <li><strong>PhysHlth:</strong> days of physical illness last 30 days [0-30]</li>\n",
        "  <li><strong>DiffWalk:</strong> difficulty walking or climbing stairs [0-1]</li>\n",
        "  <li><strong>Stroke:</strong> already had a stroke: [0-1]</li>\n",
        "  <li><strong>HighBP:</strong> high blood pressure [0-1]</li>\n",
        "</ul>\n",
        "</p>\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vtcsuk4XQ7VE"
      },
      "outputs": [],
      "source": [
        "# With this command, you can download the data we will use on the server. Simply run the cell.\n",
        "!wget https://raw.githubusercontent.com/Dichopsis/ML-TP-ESBS/main/diabetes_data_TP.csv\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiDYaJWBQ7VF"
      },
      "source": [
        "# A tip before starting\n",
        "The most useful tip I can give your for this pratical but also for code in general is that **everytime you are lost on how to use a function or what function to use, simply google \"library_of_the_function name_of_the_function\" and click this first result**. You will almost always have an example on how to use the function and all the parameters needed.  \n",
        "For example:\n",
        "* \"scikit train_test_split\" -> https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_52awJ5Q7VG"
      },
      "source": [
        "# Part 1 - Data Exploration\n",
        "### Tasks: explore the data\n",
        "* import the data\n",
        "* count the number of persons and visualize the 5 first line\n",
        "* see the diabetes vs non-diabetes person ratio\n",
        "* plot a box-plot of the BMI and an histogram of the smokers\n",
        "### Questions\n",
        "1. How many persons are in this dataset ? (rows). How many features/measurements ? (columns)\n",
        "2. What is the percentage of persons with diabetes ?\n",
        "3. What is the median BMI of the persons in the dataset ? (approximately with BoxPlot)\n",
        "4. Is there more non-smoker or smokers in the dataset ? (Histogram)  \n",
        "\n",
        "![image](https://i.imgur.com/xLoQ27w.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfIjt-uvQ7VG"
      },
      "outputs": [],
      "source": [
        "# Pandas is the main library for data manipulation in Python\n",
        "import pandas as pd\n",
        "# Matplotlib is the main library for data visualization in Python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import the data (df variable) using pd.read_csv()\n",
        "df =\n",
        "\n",
        "print(\"=== QUESTION 1 ===\")\n",
        "# Print the shape of the dataframe and the head using df.shape and df.head()\n",
        "print(df. ....)\n",
        "df. ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jD6KlthQ7VG"
      },
      "outputs": [],
      "source": [
        "print(\"=== QUESTION 2 ===\")\n",
        "# Use value_counts() on the \"stroke\" column. To select a column, use df[\"column_name\"]\n",
        "# Tip: set the \"normalize\" parameter to True in the value_count() function !\n",
        "print(df[\"column_name\"]. ...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnTosuPzQ7VH"
      },
      "outputs": [],
      "source": [
        "print(\"=== QUESTION 3 ===\")\n",
        "# Make a boxplot of the \"BMI\" column using the boxplot() function on the dataframe with the parameter \"column\"=\"column_name\"\n",
        "boxplot = df. ....\n",
        "# We use plt.show() to display the boxplot\n",
        "plt.show(boxplot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0XfS1HLQ7VH"
      },
      "outputs": [],
      "source": [
        "print(\"=== QUESTION 4 ===\")\n",
        "# Make a histogram of the \"Smoker\" column using the hist() function on the dataframe with the parameter \"column\"=\"column_name\"\n",
        "hist = df. ...\n",
        "# We use plt.show() to display the histogram\n",
        "plt.show(hist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr2o7fvzQ7VH"
      },
      "source": [
        "# Part 2 - Data Processing and splitting\n",
        "### Tasks: encode the data to be usable for training by a ML algorithm\n",
        "* analyze what columns we need to process and modify them to do machine-learning !\n",
        "* split our dataset between a training dataset and a test dataset\n",
        "\n",
        "### Questions\n",
        "1. What columns are categorical data, what columns are numeric.\n",
        "2. What columns are already ready to be used and needs no change.\n",
        "3. What type of processing do you need to do on categorical data and why\n",
        "4. What type of processing do you need to do on numeric data and why\n",
        "5. What columns contains missing data ? What type of processing do you need to do in this case.\n",
        "\n",
        "![image](https://i.imgur.com/HbEoLeR.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTrSRxJaQ7VI"
      },
      "outputs": [],
      "source": [
        "print(\"=== QUESTION 1 to 4 ===\")\n",
        "# You should be able to answer the questions 1 to 4 simply with the table below (df.head())\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7FyfUHVQ7VI"
      },
      "outputs": [],
      "source": [
        "print(\"=== QUESTION 5 ===\")\n",
        "# You should be able to answer the questions 5 simply with the table below (df.info())\n",
        "# (remember how many rows there are in the dataframe)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piJaBP7eQ7VI"
      },
      "outputs": [],
      "source": [
        "# We will now process theses columns to make them usable\n",
        "# From Scikit learn we import the tools we will need\n",
        "# OrdinalEncoder for the categorical variables\n",
        "# StandardScaler for the numerical variables\n",
        "# IterativeImputer for the missing values\n",
        "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "# NumPy is the main library for array manipulation in Python\n",
        "import numpy as np\n",
        "\n",
        "# Create lists with columns names for each processing categories\n",
        "columns_nothing_to_do = [\"...\"] # 10 Columns\n",
        "categorical_columns_to_process = [\"...\"] # 4 Columns\n",
        "numerical_columns_to_process = [\"....\"] # 4 Columns\n",
        "\n",
        "\n",
        "# Categorical data to numeric: do one hot encoding ( OrdinalEncoder(), .fit_transform() )\n",
        "# I give you the OrdinalEncoder example. Do something similar for the Scaling of numerics data !\n",
        "X_cat = df[categorical_columns_to_process]\n",
        "X_cat_enc = OrdinalEncoder().fit_transform(X_cat)\n",
        "\n",
        "# Numeric data: do scaling (-1,+1) (StandardScaler() and .fit_transform())\n",
        "X_num = df[numerical_columns_to_process]\n",
        "X_num_scaled =\n",
        "\n",
        "# Nothing-To-Do Cols: just select the data corresponding with df[name_of_the_list] and store it into X_nothing_to_do\n",
        "# Then convert X_nothing_to_do to numpy array with .to_numpy()\n",
        "X_nothing_to_do =\n",
        "X_nothing_to_do =\n",
        "\n",
        "# Combine the X_cat_enc, X_num_scaled, and X_nothing_to_do data into a single array (array_data)\n",
        "# using np.concatenate((array1, array2, array3), parameters=value) ; TIP: Care for the axis parameter (axis=1)\n",
        "array_data =\n",
        "\n",
        "# Finally handle Missing Data: Use imputer to predict them ( IterativeImputer() and .fit_transform() on array_data just as before\n",
        "# This time you do it on all the data in array_data, no need to select columns\n",
        "array_data =\n",
        "\n",
        "# You can recreate a DataFrame for pretty printing, but this is optional. I give you the code it's a bit ugly.\n",
        "df_data = pd.DataFrame(data=array_data, columns=list(X_cat.columns) + list(X_num.columns) + list(df[columns_nothing_to_do].columns))\n",
        "df_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGQxBtHKQ7VJ"
      },
      "source": [
        "### Tasks: Split our dataset between train and test set\n",
        "* split our processed data into train and test dataset\n",
        "\n",
        "### Questions\n",
        "1. What train/test ratio should you use.\n",
        "2. How many entries are in your train dataset and in your test dataset.\n",
        "3. Verify that you have the same diabetes / diabetes ratio between train and test dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNCquPxuQ7VJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# From your numpy array that you created, separate the X columns (features: multiples columns) and the Y column (label: last column) in two variables\n",
        "# Call them X and Y, X contains all rows and all the columns except the last one, Y contains only the last column from the array_data variable.\n",
        "X = array_data[...]\n",
        "Y = array_data[...]\n",
        "\n",
        "# Use train_test_split() using X and Y with the ratio you selected. The parameter test_size= is the ratio of the test set.\n",
        "X_train, X_test, Y_train, Y_test = ...\n",
        "\n",
        "# Print X_train and X_test shape, use np.unique() on Y_train and Y_test to get the diab vs no diab ratio\n",
        "# (To normalize the ratio you can use the parameter return_counts with the True value)\n",
        "print(...)\n",
        "print(....)\n",
        "print(np.unique(...))\n",
        "print(np.unique(...))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBySrHnrQ7VJ"
      },
      "source": [
        "# Part 3 - Create your machine-learning model\n",
        "### Tasks: Choose a model and do basic evaluation\n",
        "\n",
        "Select from scikit-learn a model and train it (.fit()) with the train data. Then calculate the accuracy of the model on the test data (.score()). Plot the confusion matrix of the test data classification.  \n",
        "All ML algorithm in Sci-kit can be found here (1.1 to 1.11) https://scikit-learn.org/stable/supervised_learning.html  \n",
        "Personally I will use a RandomForest or a Decision Tree for example.\n",
        "\n",
        "### Questions:\n",
        "\n",
        "1. Which model did you choose and why ? Have you set any particular (hyper)parameters ?\n",
        "2. What accuracy-score do you get and what conclusion can you take ?\n",
        "3. What do you observe on the confusion matrix and what conclusion can you take ?\n",
        "\n",
        "![image](https://i.imgur.com/JttOOSw.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gy19_DwkQ7VJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select a model from: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning\n",
        "\n",
        "# For example: create a clf variable containing a RandomForestClassifier() or DecisionTreeClassifier().\n",
        "# If you select another model, you will have to import it first (look above)\n",
        "# Don't hesitate to tweak its parameters as you like ! You can experiment.\n",
        "clf = ...\n",
        "# Use .fit() on clf variable with your X_train and Y_train to train the model.\n",
        "clf = ...\n",
        "\n",
        "print(\"=== QUESTION 2-3 ===\")\n",
        "# Using the .score() method of the clf variable (your trained model), print its accuracy on X_test, Y_test\n",
        "print(...)\n",
        "# Plot the confusion matrix using ConfusionMatrixDisplay.from_estimator() with clf, X_test, Y_test arguments)\n",
        "display = ...\n",
        "plt.show(display.plot())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEHSzz2RQ7VJ"
      },
      "source": [
        "### Tasks: Correct the previous issue\n",
        "\n",
        "You will now try to downsample your majority class (non-diabetes) to the level of minority class (diabetes) to have a 50/50 ratio and re-do a train/test split.  \n",
        "Then you will re-train a new model with the new ratio-corrected data and get accuracy+confusion matrix plot. Don't only predict the class, but also show the prediction probability for all data in the test set !\n",
        "\n",
        "### Questions\n",
        "\n",
        "1. What accuracy-score do you get with the new model and what conclusion can you take.\n",
        "2. What do you observe on the confusion matrix and what conclusion can you take.\n",
        "3. Did you managed to print the probability of each prediction ? What's the shape of the prediction probability output ? Is there a high variance between the different test entries in probability ?\n",
        "\n",
        "![image](https://i.imgur.com/D6iAllf.png)\n",
        "\n",
        "Graphical Recap of what we did so far and what we are going to do ! Because there is a lot of variable and it's easy to be lost.  \n",
        "  \n",
        "![image](https://i.imgur.com/I8FyMhD.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4c1o-GzQ7VJ"
      },
      "outputs": [],
      "source": [
        "# So here because it's a bit complicated I give you directly the function called downsample_majority_class() to do the downsampling.\n",
        "# But you can try to read the code and understand how it works !\n",
        "def downsample_majority_class(X, Y):\n",
        "    np.random.seed(777)\n",
        "    # Get index of rows for each class (0/1)\n",
        "    i_class0 = np.where(Y == 0)[0]\n",
        "    i_class1 = np.where(Y == 1)[0]\n",
        "    # Total Number of observations in each class\n",
        "    n_class0 = len(i_class0)\n",
        "    n_class1 = len(i_class1)\n",
        "    # For every observation of class 1, randomly select (sample) from index list of class 0 without replacement\n",
        "    i_class0_downsampled = np.random.choice(i_class0, size=n_class1, replace=False)\n",
        "    # Join together the downsampled class 0's target vector with class 1's target vector\n",
        "    Y_down = np.hstack((Y[i_class0_downsampled], Y[i_class1]))\n",
        "    X_down = np.vstack((X[i_class0_downsampled], X[i_class1]))\n",
        "    return X_down, Y_down\n",
        "\n",
        "# Call the function downsample_majority_class() on X and Y and store our new dataset in X_down, Y_down\n",
        "X_down, Y_down = ...\n",
        "\n",
        "# Re-do a train/test split (with train_test_split() ) but this time on the new X_down and Y_down data (downsampled).\n",
        "# Call the new variables X_train_down, X_test_down, Y_train_down, Y_test_down for example.\n",
        "X_train_down, X_test_down, Y_train_down, Y_test_down = ...\n",
        "\n",
        "# Recreate a model called clf_down with RandomForestClassifier() or DecisionTreeClassifier() for example.\n",
        "clf_down = ...\n",
        "# And .fit() it this time on X_train_down, Y_train_down\n",
        "clf_down = ...\n",
        "\n",
        "print(\"=== QUESTION 1-3 ===\")\n",
        "# Calculate its accuracy using the .score() methods of the clf_down variable with X_test_down, Y_test_down as arguments.\n",
        "print(...)\n",
        "# Plot the confusion matrix as before, but with clf_down, X_test_down, Y_test_down\n",
        "display2 = ...\n",
        "plt.show(display2.plot())\n",
        "\n",
        "# Print the prediction probability of the first 10 test data points using clf_down.predict_proba(X_test_down) of the model.\n",
        "clf_down. ...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I50Je9x7Q7VL"
      },
      "source": [
        "# Part 4 - Evaluate your model\n",
        "\n",
        "### Tasks: See all common metrics and evaluate both models !\n",
        "\n",
        "In this last part you will have to calculate all relevant metrics for a binary classification to compare your two models.\n",
        "Make a table containing the results for both models in terms of: accuracy, balance accuracy, F1 Score, sensitivity (recall), specificity, Precision and confusion matrix data (True Pos., True Neg., False Pos., False Neg.)  \n",
        "\n",
        "### Questions:\n",
        "\n",
        "1. Which model has the accuracy ?\n",
        "2. Do you know the difference between accuracy and balanced accuracy ? What model have the best balanced accuracy\n",
        "3. Which model has the best F1-Score and sensitivity ? Do you know how is F1-Score calculated ?\n",
        "4. Eventually, which model is better according to you based on the metrics ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbaosoogQ7VL"
      },
      "outputs": [],
      "source": [
        "# Calculate some relevant metrics for a binary classification\n",
        "# Using Scikit score functions: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
        "# Complete \"get_all_metrics\" function to calculate: Accuracy (ac), Balanced Accuracy (bac), recall (re), precision (pr),  F1 Score (f1)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def get_all_metrics(clf, X_test, Y_test):\n",
        "\t\"\"\"\"Function that returns all the metrics for a given classifier and test data\"\"\"\n",
        "\t# Use classifier to predict the Y label of X_test data with .predict()\n",
        "\ty_pred = clf.predict(X_test)\n",
        "\n",
        "\t# Confusion Matrix results\n",
        "\t# true negative, false positive, false neg, true pos\n",
        "\ttn, fp, fn, tp = confusion_matrix(Y_test, y_pred).ravel()\n",
        "\n",
        "\t# Calculate the metrics: accuracy, balanced accuracy, recall, precision, F1-score. Accuracy and Specificity are already done.\n",
        "\t# By calling the metrics we imported above on Y_testd and y_pred\n",
        "\tac = ...\n",
        "\tbac = ...\n",
        "\tre = ...\n",
        "\tpr = ...\n",
        "\tf1 = ...\n",
        "\t# specificity (divided by 0 in some case)\n",
        "\ttry: sp = tn/float(tn+fp)\n",
        "\texcept: sp = 0\n",
        "\n",
        "\treturn [bac, ac, f1, re, sp, pr, tp, tn, fp, fn]\n",
        "\n",
        "# Here is a function that create a table with the results of the two models, you don't need to touch it, just call it in the next cell.\n",
        "def make_table_clf(clf, clf_down, X_test, Y_test, X_test_down, Y_test_down):\n",
        "\t\"\"\"\"Use the get_all_metrics function to make a table to compare two models\"\"\"\n",
        "\tresults = get_all_metrics(clf, X_test, Y_test)\n",
        "\tresults_down = get_all_metrics(clf_down, X_test_down, Y_test_down)\n",
        "\tdf = pd.DataFrame([[i for i in results], [j for j in results_down]], columns=[\"Balanced-Accuracy\", \"Accuracy\", \"F1-Score\", \"Sensitivity (Recall)\", \"Specificity\", \"Precision\", \"TP\",\" TN\", \"FP\", \"FN\"], index=[\"CLF\",\"CLF DownSampled\"])\n",
        "\treturn df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N1UMgXNQ7VM"
      },
      "outputs": [],
      "source": [
        "print(\"=== QUESTION 1-4 ===\")\n",
        "# Use the make_table_clf function to compare the metrics of both models !\n",
        "# You need to call it on the clf, clf_down, X_test, Y_test, X_test_down, Y_test_down variables.\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "401Cv2MyQ7VM"
      },
      "source": [
        "# Part 5 - Can we improve the model further ?\n",
        "\n",
        "# Context:\n",
        "**Can we improve the confidence on the performance measure of the model? And can we actually improve its performances ?**\n",
        "* **Cross-Validation**  \n",
        "Instead of a simple Test/Train split, we will do cross-validation. This means that we will train multiple models with different splits, so that all data have been used for training and all for testing. Then we will average the results of all models.\n",
        "For example, if we do a 80/20% train/test split, then we would need to do a 5-fold cross-validation so that each data has been in the test-set at least once.\n",
        "* **Hyper-Parameters Tuning**  \n",
        "Each machine-learning algorithm (for example Random Forest) has a certain number of parameters that can be changed. For example, the number of trees used to learn for the data, the maximum depth of the tree, the criterion setting ...  \n",
        "Changing those parameters to find the optimal ones can actually improve the performance of the model!  \n",
        "Here we will learn to do hyper-parameters tuning (i.e. searching for the optimal parameters of your model) to improve the performances using what we call a \"Grid Search\"  \n",
        "\n",
        "(Left: cross-validation, Right: hyper-parameters tuning)\n",
        "![image](https://i.imgur.com/ybnTDz1.png)\n",
        "\n",
        "# Task: Fine tune our model to improve its performances\n",
        "* Create a param_grid dictionary variable that will contain all the hyperparameters to test\n",
        "* Call GridSearchCV with our model, the param_grids, and the number of cross-val folds to test all combinations\n",
        "* Print what were the best parameters and the final performance score\n",
        "\n",
        "# Questions:\n",
        "1. What is the point of cross-validation? Does it increase performance? If not, what is it useful for? What is the maximum of fold you theoretically do for a cross-validation ?\n",
        "2. For GridSearch, you need to choose what metric you want to maximize, what would you choose?\n",
        "3. What are the parameters for the try with the best metric?\n",
        "4. Compare the metric of the best model found using GridSearch to the metrics of the previous clf_down, is it better now ?\n",
        "\n",
        "**Warnings**\n",
        "\n",
        "This can be a pretty long task depending on your computer and the ML algorithm you chose because the program will train and test as many models as parameter combinations multiplied by the number of cross-validation you selected. If it's too long, you can cancel, lower the CV, lower the number of parameters you test or choose a less expensive ML algorithm (for example decision tree, logistic regression...)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fw8A6TMXQ7VM"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "# Create a param_grid dictionary with the parameters you want to test for your model.\n",
        "# For example, if I'm using a KNeighborsClassifier, I would visit https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
        "# To see all the parameters I can tune, and I would create a dictionary with all the parameters I want to test.\n",
        "# Such as:\n",
        "# param_grid = {\n",
        "# 'n_neighbors': [1, 3, 5, 7, 9],\n",
        "# 'weights': ['uniform', 'distance'],\n",
        "# 'metric': ['euclidean', 'manhattan']\n",
        "# }\n",
        "\n",
        "# Fill the \"...\" with a 3 values for n_estimators and max_depth for RandomForestClassifier(). Check official documentation.\n",
        "param_grid = {\n",
        "            'n_estimators': [....],\n",
        "            'max_depth': [....],\n",
        "            'min_samples_leaf': [1, 2, 4],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'criterion': ['gini', 'entropy', \"log_loss\"]\n",
        "            }\n",
        "# Then call GridSearchCV() with your model (KNeighborsClassifier() for example), the param_grid, the number of cross-val folds (cv=NUMBER) and the scoring metric you prefer (scoring=\"your_metric\").\n",
        "# Here is the documentation: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
        "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='balanced_accuracy')\n",
        "# And Call the .fit() method on the grid_search variable with the X_down, Y_down variables as arguments.\n",
        "grid_search.fit(X_down, Y_down)\n",
        "# The code below will print the best parameters and the best score along with the results of all the tests.\n",
        "df_gridsearch = pd.DataFrame(grid_search.cv_results_)\n",
        "print(\"Best Metrics: \", grid_search.best_score_)\n",
        "print(\"Best Parameters: \", grid_search.best_params_)\n",
        "df_gridsearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcmJL1TLQ7VM"
      },
      "source": [
        "# Final Graph of what you did during this practical ðŸ¥³\n",
        "![image.png](https://i.imgur.com/QcBKO3Q.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbIwnEvdQ7VN"
      },
      "source": [
        "# Further Resources\n",
        "## General Python\n",
        "* Probably the best website to learn about some Python lib https://calmcode.io/\n",
        "* Made by somebody that I like a lot (Vincent D. Warmerdam) and his blog https://koaning.io/\n",
        "* **Promo: website: https://cmeyer.fr/**\n",
        "* Visualisation in Python https://www.python-graph-gallery.com/\n",
        "\n",
        "# AI & Data Specific\n",
        "* Google's Class lists and ressources on ML https://developers.google.com/machine-learning/crash-course\n",
        "* Wanna learn AI ? Check this Roadmap https://i.am.ai/roadmap\n",
        "* Public Dataset to create projects https://github.com/awesomedata/awesome-public-datasets\n",
        "* Machine Learning Competitions with Prize https://www.kaggle.com/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TP_ML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:49:35) \n[GCC 10.4.0]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "4332e181933f75a37612bde606e3f6a5973d00f8b184453988c147666871ccc4"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}